# ğŸš€ Cookiecutter ML Pipeline Template

A machine learning pipeline template for starting a machine learning projects including preprocessing, model selection and training, and model inference.

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [Template Options](#template-options)
4. [Project Structure](#project-structure)
5. [Framework Architecture](#framework-architecture)
6. [Best Practices](#best-practices)

## Overview

This framework follows three main steps in the pipeline: **preprocess**, **training**, and **inference**. The design is flexible, allowing users to choose and implement their own logic for each step. The template is suitable for both classical machine learning and deep learning workflows, and is intended for research and experimentation rather than production.

**Key Features:**
- Modular architecture for preprocess, training, and inference
- Abstract base classes for extensibility
- Versioned results and experiment tracking
- Hydra-based configuration management
- Optuna integration for hyperparameter optimization
- Example option for quick exploration

## Quick Start

### 1. Install Cookiecutter
```bash
pip install cookiecutter
```

### 2. Generate Project
```bash
cookiecutter https://github.com/ItamarEfrati/PipeLion.git --directory ml-project
```

## Template Options

The template includes a very simple example pipeline. To use the example, first run the sample data generation script provided in the template. This will create example data for you to explore the pipeline. The example implementation demonstrates the full pipeline (preprocess, training, inference) and is intended for research and learningâ€”not for production use. When generating the template, you can choose to include or exclude these examples.

**With examples:**
- Example study, dataloader, inference handler
- Sample data and configs
- Data generation script to create example data

**Without examples:**
- Only the abstract base classes and core structure
- No sample data or example implementations

## Project Structure

```
ml_project/
â”œâ”€â”€ ğŸ“„ requirements.txt                 # âœ… Core dependencies
â”œâ”€â”€ ğŸ“ src/                            # Main source code
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py                 # âœ… Package initialization  
â”‚   â”œâ”€â”€ ğŸ“„ main.py                     # âœ… Entry point orchestrator
â”‚   â”œâ”€â”€ ğŸ“„ orchestrator.py             # âœ… Pipeline coordination
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ training/                   # Training pipeline components
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py             # âœ… Always included
â”‚   â”‚   â”œâ”€â”€ ğŸ“ dataloaders/            # Data loading abstractions
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py         # âœ… Always included
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ example_dataloader.py # ğŸ¯ CSV-based example
â”‚   â”‚   â””â”€â”€ ğŸ“ hyperparameters_tuning/ # Study management
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ __init__.py         # âœ… Always included
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ abstract_study.py   # âœ… Base Study interface
â”‚   â”‚       â””â”€â”€ ğŸ“ user_studies/       # Your custom studies
â”‚   â”‚           â”œâ”€â”€ ğŸ“„ __init__.py     # âœ… Always included
â”‚   â”‚           â””â”€â”€ ğŸ“„ example_study.py # ğŸ¯ RandomForest study
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ inference/                  # Inference pipeline
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py             # âœ… Always included
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ abstract_inference.py   # âœ… Base inference interface
â”‚   â”‚   â””â”€â”€ ğŸ“„ example_inference.py    # ğŸ¯ Working inference example
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ preprocess/                 # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py             # âœ… Always included
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_processor.py       # âœ… Core preprocessing logic
â”‚   â”‚   â”œâ”€â”€ ğŸ“ feature_arrangement/    # Feature engineering
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py         # âœ… Always included
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ abstract_feature_arranger.py # âœ… Base interface
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ example_feature_arranger.py  # ğŸ¯ Example implementation
â”‚   â”‚   â””â”€â”€ ğŸ“ micro_services/         # Modular processing services
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ __init__.py         # âœ… Always included
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ micro_service.py    # âœ… Base service class
â”‚   â”‚       â””â”€â”€ ğŸ“ user_implementations/ # Your custom services
â”‚   â”‚           â”œâ”€â”€ ğŸ“„ __init__.py     # âœ… Always included
â”‚   â”‚           â”œâ”€â”€ ğŸ“„ feature_engineer.py    # ğŸ¯ Example feature eng.
â”‚   â”‚           â””â”€â”€ ğŸ“„ statistics_calculator.py # ğŸ¯ Example stats
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ utils/                      # Shared utilities
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py             # âœ… Always included
â”‚       â””â”€â”€ ğŸ“„ constants.py            # âœ… Global constants
â”‚
â”œâ”€â”€ ğŸ“ config/                         # Configuration management
â”‚   â”œâ”€â”€ ğŸ“„ config.yaml                 # âœ… Main configuration
â”‚   â”œâ”€â”€ ğŸ“„ hydra/default.yaml          # âœ… Hydra settings
â”‚   â”œâ”€â”€ ğŸ“„ inference_config.yaml       # ğŸ¯ Inference configuration
â”‚   â”œâ”€â”€ ğŸ“„ train_config.yaml           # ğŸ¯ Training configuration
â”‚   â”œâ”€â”€ ğŸ“ dataloader/                 # DataLoader configs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ example_dataloader_train.yaml      # ğŸ¯ Examples
â”‚   â”‚   â””â”€â”€ ğŸ“„ example_dataloader_inference.yaml  # ğŸ¯ Examples
â”‚   â”œâ”€â”€ ğŸ“ study/                      # Study configurations
â”‚   â”‚   â””â”€â”€ ğŸ“„ example_study.yaml      # ğŸ¯ Example study config
â”‚   â”œâ”€â”€ ğŸ“ inference/                  # Inference configurations
â”‚   â”‚   â””â”€â”€ ğŸ“„ example_inference.yaml  # ğŸ¯ Example inference config
â”‚   â””â”€â”€ ğŸ“ preprocess/                 # Preprocessing configs
â”‚       â”œâ”€â”€ ğŸ“„ example_preprocess_train.yaml     # ğŸ¯ Examples
â”‚       â””â”€â”€ ğŸ“„ example_preprocess_inference.yaml # ğŸ¯ Examples
â”‚
â”œâ”€â”€ ğŸ“ data/                           # Data organization
â”‚   â”œâ”€â”€ ğŸ“ raw_data/                   # Original datasets
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ labels.csv              # ğŸ¯ Example labels
â”‚   â”‚   â”œâ”€â”€ ğŸ“ Source_A/               # ğŸ¯ Example data source
â”‚   â”‚   â”œâ”€â”€ ğŸ“ Source_B/               # ğŸ¯ Example data source  
â”‚   â”‚   â””â”€â”€ ğŸ“ Source_C/               # ğŸ¯ Example data source
â”‚   â”œâ”€â”€ ğŸ“ preprocessed/               # Processed datasets
â”‚   â”‚   â”œâ”€â”€ ğŸ“ features/               # ğŸ¯ Example processed features
â”‚   â”‚   â”œâ”€â”€ ğŸ“ outcomes_Source_A/      # ğŸ¯ Example outcomes
â”‚   â”‚   â”œâ”€â”€ ğŸ“ outcomes_Source_B/      # ğŸ¯ Example outcomes
â”‚   â”‚   â””â”€â”€ ğŸ“ outcomes_Source_C/      # ğŸ¯ Example outcomes
â”‚   â”œâ”€â”€ ğŸ“ for_modeling/               # Ready-for-training data
â”‚   â”‚   â””â”€â”€ ğŸ“ features_1/             # ğŸ¯ Example modeling data
â”‚   â””â”€â”€ ğŸ“ for_inference/              # Inference-ready data
â”‚       â””â”€â”€ ğŸ“ Source_C/               # ğŸ¯ Example inference data
â”‚
â”œâ”€â”€ ğŸ“ assets/                         # Generated assets
â”‚   â””â”€â”€ ğŸ“ results/                    # Study results and models
â”‚       â””â”€â”€ ğŸ“ example_study/          # ğŸ¯ Example study outputs
â”‚           â”œâ”€â”€ ğŸ“„ example_study.db    # Optuna database
â”‚           â””â”€â”€ ğŸ“ ver_1/              # Versioned results
â”‚               â”œâ”€â”€ ğŸ“„ best_model.pkl  # Trained model
â”‚               â”œâ”€â”€ ğŸ“ inference/      # Inference results
â”‚               â””â”€â”€ ğŸ“ run_results/    # Training metrics
â”‚
â”œâ”€â”€ ğŸ“ scripts/                        # Utility scripts
â”‚   â””â”€â”€ ğŸ“„ generate_sample_data.py     # ğŸ¯ Sample data generator
â”‚
â””â”€â”€ ğŸ“ notebooks/                      # Jupyter notebooks (empty)
```

**Legend:**
- âœ… **Always included** - Core framework files
- ğŸ¯ **Examples only** - Included when `include_examples: "yes"`

## Framework Architecture

The framework is organized around abstract modules for each pipeline step: preprocess, training, and inference. Each module provides a base interface, and you are free to implement your own logic for any step. You can use any ML or deep learning library (e.g., scikit-learn, PyTorch, TensorFlow) and design your own workflow. The framework does not enforce any specific algorithm or data formatâ€”it's up to you to decide how to structure your pipeline.

**Core abstractions:**
- Preprocess: Define how raw data is transformed and features are engineered
- Training: Implement your own study logic for model training and hyperparameter optimization
- Inference: Create custom handlers for model loading and prediction

You choose what to implement and how to connect the steps. The framework provides the structure and flexibility for research workflows.

## Best Practices

### 1. **Configuration Management**
- Keep configs small and focused
- Use Hydra composition for complex scenarios
- Override parameters via command line when needed
- Version control your configuration files

### 2. **Code Organization**
- One class per file for major components
- Use descriptive names for studies and handlers
- Keep business logic separate from framework code
- Add docstrings to all public methods

### 3. **Data Management**
- Organize data by source and processing stage
- Use consistent naming conventions
- Keep raw data immutable
- Document data transformations

### 4. **Experiment Tracking**
- Use meaningful study names
- Let the framework handle versioning
- Save important artifacts (plots, reports)
- Document significant findings

### 5. **Testing Strategy**
- Test with small datasets first
- Validate preprocessing pipelines separately  
- Use example components as templates
- Monitor training metrics closely